Simulation
	Find 3 benchmark data sets and compare the methods.
	Pick 5-10(-ish) supervised classifiers.
	Compare the supervised classifiers in the query-by-bagging approach with say B = 10, 50, and 100.
		Compare them with each disagreement measure.
		In webnotebook, create a plot for each B.
	Also, compare the 10 classifiers in the query-by-committee approach.
		Compare them with each disagreement measure.
	Need a text classification data set in the actual paper since that is the application of most AL methods.
	At some point, need to find a benchmark data set with at least one time-dependent covariate.
		Do the methods still work with time-dependent covariates?
		
Outline a vignette
	Look at some examples in Journal of Statistical Software for guides
Copy vignette outline to a webnotebook
Fill in some of the details in webnotebook
Move code from inst/simulations/ to ProjectTemplate?


Throw an error if the "uncertainty" measure is incorrect in uncert_sampling().
 Write a unit test to catch this.
Throw an error if the "disagreement" measure is incorrect in query_by_bagging().
 Write a unit test to catch this.
When returning results from the active learning functions, consider the following:
	Rather than returning the 'obs_uncertainty' or something just as obscure,
	consider naming this the same as the uncertainty or disagreement measure.
	Example: If we are considering 'entropy' in uncert_sampling(), then return
	'entropy' as a list component instead of 'obs_uncertainty'

The R package needs a spiffy new name.

Add note that we may be able to weight the Bayesian information density with a power prior.
